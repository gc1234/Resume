{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custem_training_loop.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsR-ofrHplob",
        "colab_type": "code",
        "outputId": "357b6d2e-0836-4464-e39a-226d42335727",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlzKIfxfpqxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load a toy dataset for the sake of this example\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the data (these are Numpy arrays)\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Add a channels dimension\n",
        "x_train = x_train[..., tf.newaxis].astype('float32')\n",
        "x_test = x_test[..., tf.newaxis].astype('float32')\n",
        "\n",
        "y_train = y_train.astype('float32')\n",
        "y_test = y_test.astype('float32')\n",
        "\n",
        "val_size = 12000\n",
        "train_size = len(x_train) - val_size\n",
        "\n",
        "# Reserve 12,000 samples for validation\n",
        "x_val = x_train[-val_size:]\n",
        "y_val = y_train[-val_size:]\n",
        "x_train = x_train[:-val_size]\n",
        "y_train = y_train[:-val_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xuLbTQ8RygA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  activation_fn = tf.nn.elu\n",
        "  regularization_fn = tf.keras.regularizers.l2\n",
        "  #n_filters = 256\n",
        "  #n_kernals = 3\n",
        "  regularization_rate = 1e-4\n",
        "\n",
        "  inputs = tf.keras.Input(shape=(28, 28, 1), name='original_img')\n",
        "  x = tf.keras.layers.Conv2D(32, 3, activation=activation_fn, kernel_regularizer=regularization_fn(regularization_rate))(inputs)\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  x = tf.keras.layers.Dense(128, activation=activation_fn, kernel_regularizer=regularization_fn(regularization_rate))(x)\n",
        "  predictions = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "  model = tf.keras.Model(inputs, predictions, name='mnist_cnn')\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alUEE_9qSIEG",
        "colab_type": "code",
        "outputId": "31b755e1-e6a2-4642-fccb-4a52d2a02235",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "tf.random.set_seed(0)\n",
        "\n",
        "model_name = \"minst_model\"\n",
        "try:\n",
        "  model = tf.keras.models.load_model(model_name)\n",
        "  print(\"Loaded model.\")\n",
        "except Exception:\n",
        "  model = build_model()\n",
        "  model.save(model_name)\n",
        "  print(\"Built model.\")\n",
        "\n",
        "#model = build_model()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model.\n",
            "Model: \"mnist_cnn\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "original_img (InputLayer)    [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 21632)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               2769024   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 2,770,634\n",
            "Trainable params: 2,770,634\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghJ58o-E7Lgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_step = tf.Variable(0, trainable=False)\n",
        "lr_boundaries = [400000, 600000]\n",
        "lr_values = [1e-2, 1e-3, 1e-4]\n",
        "learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(lr_boundaries, lr_values)\n",
        "# Later, whenever we perform an optimization step, we pass in the step.\n",
        "lr_schedule = learning_rate_fn(lr_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBiRvwpQp0pd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate an optimizer to train the model.\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9)\n",
        "# Instantiate a loss function.\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "val_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "# Prepare the metrics.\n",
        "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "# Prepare the training dataset.\n",
        "batch_size = 32\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "# Prepare the validation dataset.\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8BdqtUwyezR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(x_batch_train, y_batch_train):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(x_batch_train)\n",
        "    loss_value = loss_fn(y_batch_train, logits)\n",
        "    \n",
        "    # Add extra losses created during this forward pass:\n",
        "    loss_value += sum(model.losses)\n",
        "  \n",
        "  grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "  return logits, loss_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_lmJI1_4LhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@tf.function\n",
        "def validation_loop(val_dataset):\n",
        "  # Run a validation loop at the end of each epoch.\n",
        "  for x_batch_val, y_batch_val in val_dataset:\n",
        "    val_logits = model(x_batch_val)\n",
        "    # Update val metrics\n",
        "    val_acc_metric(y_batch_val, val_logits)\n",
        "  val_acc = val_acc_metric.result()\n",
        "  val_acc_metric.reset_states()\n",
        "  print('Validation acc: %s' % (float(val_acc),))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koXGhFZI2OSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
        "\n",
        "manager = tf.train.CheckpointManager(\n",
        "    checkpoint, directory=checkpoint_dir, max_to_keep=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klhYmsS_qSB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_dataset, epochs):\n",
        "  # Iterate over epochs.\n",
        "  #status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "  status = checkpoint.restore(manager.latest_checkpoint)\n",
        "\n",
        "  print(\"Train on {} samples, validate on {} samples\".format(val_size, train_size))\n",
        "  for epoch in range(epochs):\n",
        "    print('Epoch %d/%d' % (epoch + 1, epochs))\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "      (logits, loss_value) = train_step(x_batch_train, y_batch_train)\n",
        "      \n",
        "      # Update training metric.\n",
        "      train_acc_metric(y_batch_train, logits)\n",
        "\n",
        "      # Log every 200 batches.\n",
        "      if step % 100 == 0:\n",
        "        print ('\\r{}/{} - train_loss: {}'.format((step + 1) * batch_size, train_size, float(loss_value)), end='')\n",
        "          #print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
        "          #print('Seen so far: %s samples' % ((step + 1) * batch_size))\n",
        "\n",
        "    # Display metrics at the end of each epoch.\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print ('\\n{}/{} - train_loss: {} - train_acc: {}'.format((step + 1) * batch_size, train_size, float(loss_value), float(train_acc)))\n",
        "    #print('Training acc over epoch: %s' % (float(train_acc),))\n",
        "    # Reset training metrics at the end of each epoch\n",
        "    train_acc_metric.reset_states()\n",
        "\n",
        "    validation_loop(val_dataset)\n",
        "\n",
        "    # Save the model every 15 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      print(\"Saving model to %s\" % (checkpoint_dir,))\n",
        "      #checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "      manager.save()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ0RGdoQKj7I",
        "colab_type": "code",
        "outputId": "99388b58-6711-4406-be30-1bc58c4f09d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "train(train_dataset, epochs=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12000 samples, validate on 48000 samples\n",
            "Epoch 1/5\n",
            "44832/48000 - train_loss: 0.020668035373091698\n",
            "48000/48000 - train_loss: 0.02058146707713604 - train_acc: 0.9999791383743286\n",
            "Validation acc: 0.9807500243186951\n",
            "Epoch 2/5\n",
            "44832/48000 - train_loss: 0.020255522802472115\n",
            "48000/48000 - train_loss: 0.020419897511601448 - train_acc: 1.0\n",
            "Validation acc: 0.981333315372467\n",
            "Epoch 3/5\n",
            "44832/48000 - train_loss: 0.01914372481405735\n",
            "48000/48000 - train_loss: 0.018081573769450188 - train_acc: 1.0\n",
            "Validation acc: 0.981249988079071\n",
            "Epoch 4/5\n",
            "44832/48000 - train_loss: 0.01857433095574379\n",
            "48000/48000 - train_loss: 0.017451060935854912 - train_acc: 1.0\n",
            "Validation acc: 0.9815000295639038\n",
            "Epoch 5/5\n",
            "44832/48000 - train_loss: 0.016611458733677864\n",
            "48000/48000 - train_loss: 0.01660006493330002 - train_acc: 0.9999791383743286\n",
            "Validation acc: 0.9815833568572998\n",
            "Saving model to ./training_checkpoints\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EcKMXUd7ujc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@tf.function\n",
        "def evaluate(data, labels):\n",
        "  predictions = model(data)\n",
        "  t_loss = loss_fn(labels, predictions)\n",
        "\n",
        "  test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)\n",
        "  return float(test_loss.result()), float(test_accuracy.result())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXeq4Ey6NaX-",
        "colab_type": "code",
        "outputId": "218150bc-2810-4896-e3e9-5747f99bc0e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.07315120846033096, 0.982200026512146)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}